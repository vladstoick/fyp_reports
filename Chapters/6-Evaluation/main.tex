\chapter{Evaluation}

\section{Comparing the final application with requirements}

\section{Comparing the application with existing tools}

When comparing the functionality of the application built, the most suitable comparison would be one with XData. While a comparison could also be done with commercial tools or tutor-style applications, the comparison with XData is more relevant as both XData and our tool provide a different, and arguably better, grading algorithm that these tools.

The most important comparison with XData can be done on the basis of two criteria: the canonicalization process and the grading algorithm. One important aspect to keep in mind is that while XData also supports sub-queries to certain degree, our application does not have any form of support for sub-queries.

The canonicalization process is very similar in both apps. The improvements done in our project are fairly limited with only two canonicalizations added. Even with these two added transformations, the reality is no liitations described in section \ref{ch:lit:sec:can:subsec:limit} are still present. In addition, these two transformations do not make any progress on solving the existing issues with the process described in section.

On the other hand, the implement grading algorithm builds on top of the work done by XData. The newly implemented Boolean component comparison is a new addition that will ensure even more accurate grading. In addition, the algorithm is also able to partially grade two components that do not perfectly match. This will ensure that small mistakes such as using a \mintinline{mysql}{>} instead of \mintinline{mysql}{>=} do not get fully penalized.

In addition to the grading algorithm, XData provides some important additional features:
\begin{itemize}
    \item Support for multiple RDBMS: XData has support for Oracle DB, Microsoft SQL, Postgres (it does not have support for MySQL which is used in our project). By allowing support for multiple RDBMS, XData has the potential of being deployed in more university courses.
    \item XData implements a data generation algorithm that removes the need of seeding dummy data from the teacher. XData automatically creates a set of data built based on the teacher's correct query. This feature should prevent situations where two queries returns the same result even if they are different. According to \cite{lit:xdata_d}, this data generation has been shown to outperform fixed data-sets.
\end{itemize}

\section{Testing the application against exercies from existing sources}
Although software testing is an important part of testing any application, a more important evaluation part is looking at how the application performs against real-life examples. For this, we have tested the application against different assignment from various sources. We looked at exercises from the following two sources.

\begin{itemize}
    \item Exercises from \textit{Database System Concepts, 5th edition} written by Abraham Silberschatz, Henry F. Korth, S. Sudarshan.
    \item Exercises from Hackerrank \texttt{SQL} course.
\end{itemize}

We tried to understand how will our application be able to handle actual usage if it were to be deployed in production. We compared the ability of the app to handle the solution queries for each

\subsection{Comparing the project to a commercial tool for teaching SQL}

As mentioned in \ref{ch:lit:sec:tutor:comercial}, HackerRank provides multiple \texttt{SQL} exercises that can help a student practice his SQL abilities.

As previously described, HackerRank only provides a Boolean true/false assessment result for any submission. The functionality can be fully replicated in the application built. However, a more interesting evaluation is related to how well the computer can provide a grade and what's the quality of the components.

To assess the accuracy of the algorithm built, an evaluation of its ability to extract comparable components has been performed. We can define  a comparable component as one that was either canonicalized or cannot be rewritten in any other way. This aspect is the the most important one of the application, as it influences both the grade, as well as the hints received.

HackerRank separates their SQL course in 4 parts as following:
\begin{enumerate}
    \item \textbf{Basic Select Queries}: which contains fairly basic select queries which only query one single table. In general, they the student's ability to use \mintinline{mysql}{WHERE} clauses and to select the right columns.
    \item \textbf{Advance Select Queries}: which contains 5 advanced exercises which involves sub-queries, and the use of \mintinline{mysql}{CASE} statements.
    \item \textbf{Basic Join}: which tests student's ability to user \mintinline{mysql}{JOIN}.
\end{enumerate}

For the evaluation process we have used solutions provided by GitHub user \textit{Larkin22} made available at \url{https://github.com/Larkin22/HackerRank---SQL-Solutions}.

\subsubsection{Basic Select Queries}

The algorithm built was able to fully canonicalize the queries and extract the components

\subsection{Overview}

Overall, the algorithm performed very well in the basic queries, but had issues in the more advanced. The most prevalent issues were encountered in the following two areas:
\begin{itemize}
    \item Handling advanced queries that include sub-queries. As previously mentioned, the grading algorithm provides no support for handling sub-queries. If sub-queries are used, the application will revert back to a simple check of the results which matches HackerRank's algorithm's behavior.
    \item Handling components that cannot be easily canonicalized or compared.
\end{itemize}

It is clear that as queries go more complex, the likelihood of using sub-queries also increases which makes the algorithm unusable in these cases. However,
